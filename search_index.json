[
["index.html", "Introduction to mlr3 1 Prerequisites", " Introduction to mlr3 The mlr-org Team 2019-04-16 1 Prerequisites To get all required packages to run all examples in this book, install the mlr3book package using remotes: remotes::install_github(&quot;mlr-org/mlr3book&quot;, dependencies = TRUE) "],
["introduction.html", "2 Introduction", " 2 Introduction This is a brief introduction to the R package mlr3. First, the following building blocks are introduced: Tasks to store data and meta information about the learning task. Learners to train models and generate predictions for (new) data. Experiments introduce a class to train a Learner on a Task and provides a convenient interface to access all information of a single machine learning experiment. Next, Resampling is introduced to fit a single Learner on multiple splits of a Task and average the performance. Benchmarking goes on step further, bundling multiple resamplings for comparison. "],
["building-blocks.html", "3 Building Blocks 3.1 Tasks 3.2 Learners 3.3 Performance Measures", " 3 Building Blocks 3.1 Tasks Learning tasks encapsulate the data set and additional meta information about a machine learning problem, for example the name of the target variable for supervised problems. 3.1.1 Task Types To manually create a task from a data.frame() or data.table(), you must first determine the task type to select the respective constructor: Classification Task: Target column is labels (stored as character()/factor()) with only few distinct values. \\(\\Rightarrow\\) TaskClassif Regression Task: Target column is numeric (stored as integer()/double()). \\(\\Rightarrow\\) TaskRegr Survival Task: Target is the (right-censored) time to event. \\(\\Rightarrow\\) TaskSurv in add-on package mlr3surival Ordinal Regression Task: Target is ordinal. \\(\\Rightarrow\\) TaskOrdinal in add-on package mlr3ordinal Cluster Task: You don’t have a target but want to identify similarities in the feature space. \\(\\Rightarrow\\) Not yet implemented 3.1.2 Task Creation Let’s assume we want to create a simple regression task using the mtcars data set from the package datasets to predict the column \"mpg\" (miles per gallon). We only take the first two features here to keep the output in the following examples compact. data(&quot;mtcars&quot;, package = &quot;datasets&quot;) data = mtcars[, 1:3] str(data) ## &#39;data.frame&#39;: 32 obs. of 3 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... Next, we create the task by providing the following information: id: identifier for the task, used in plots and summaries. backend: here, we simply provide the data.frame() which is internally converted to a DataBackendDataTable. For more fine-grain control over how the data is stored internally, we could also construct a DataBackend manually. target: Column name of the target column for the regression problem. task_mtcars = TaskRegr$new(id = &quot;cars&quot;, backend = data, target = &quot;mpg&quot;) print(task_mtcars) ## &lt;TaskRegr:cars&gt; (32 x 3) ## Target: mpg ## Features (2): ## * dbl (2): cyl, disp ## ## Public: backend, cbind(), clone(), col_info, col_roles, ## data_formats, data(), droplevels(), feature_names, ## feature_types, filter(), formula(), groups, hash, head(), id, ## levels(), measures, missings(), ncol, nrow, properties, rbind(), ## replace_features(), row_ids, row_roles, select(), ## set_col_role(), set_row_role(), target_names, task_type, ## truth(), weights The print() method gives a short summary of the task: It has 32 observations, 3 columns of which 2 columns are features. 3.1.3 Predefined tasks mlr3 ships with some predefined machine learning tasks. These are stored in a Dictionary, which is a simple key-value store named mlr3::mlr_tasks. We can obtain a summarizing overview of all stored tasks by converting the dictionary to a data.table() as.data.table(mlr_tasks) ## key task_type measures nrow ncol lgl int dbl chr fct ord ## 1: bh regr regr.mse 506 19 0 3 13 0 2 0 ## 2: iris classif classif.mmce 150 5 0 0 4 0 0 0 ## 3: mtcars regr regr.mse 32 11 0 0 10 0 0 0 ## 4: pima classif classif.mmce 768 9 0 0 8 0 0 0 ## 5: sonar classif classif.mmce 208 61 0 0 60 0 0 0 ## 6: spam classif classif.mmce 4601 58 0 0 57 0 0 0 ## 7: wine classif classif.mmce 178 14 0 2 11 0 0 0 ## 8: zoo classif classif.mmce 101 17 15 1 0 0 0 0 For illustration purposes, we now retrieve the popular iris data set from mlr_tasks as a classification task: task_iris = mlr_tasks$get(&quot;iris&quot;) print(task_iris) ## &lt;TaskClassif:iris&gt; (150 x 5) ## Target: Species ## Features (4): ## * dbl (4): Petal.Length, Petal.Width, Sepal.Length, Sepal.Width ## ## Public: backend, cbind(), class_n, class_names, clone(), ## col_info, col_roles, data_formats, data(), droplevels(), ## feature_names, feature_types, filter(), formula(), groups, hash, ## head(), id, levels(), measures, missings(), ncol, negative, ## nrow, positive, properties, rbind(), replace_features(), ## row_ids, row_roles, select(), set_col_role(), set_row_role(), ## target_names, task_type, truth(), weights 3.1.4 Task API The task properties and characteristics can be queried using the task’s public member values and methods (see Task). Most of them should be self-explanatory, e.g., task_iris = mlr_tasks$get(&quot;iris&quot;) # public member values task_iris$nrow ## [1] 150 task_iris$ncol ## [1] 5 # public member methods task_iris$head(n = 3) ## Species Petal.Length Petal.Width Sepal.Length Sepal.Width ## 1: setosa 1.4 0.2 5.1 3.5 ## 2: setosa 1.4 0.2 4.9 3.0 ## 3: setosa 1.3 0.2 4.7 3.2 3.1.4.1 Retrieve Data In mlr3, each row (observation) has a unique identifier which can be either integer() or character(). These can be used to select specific rows. # iris uses integer row_ids head(task_iris$row_ids) ## [1] 1 2 3 4 5 6 # retrieve data for rows with ids 1, 51, and 101 task_iris$data(rows = c(1, 51, 101)) ## Species Petal.Length Petal.Width Sepal.Length Sepal.Width ## 1: setosa 1.4 0.2 5.1 3.5 ## 2: versicolor 4.7 1.4 7.0 3.2 ## 3: virginica 6.0 2.5 6.3 3.3 # mtcars uses the rownames of the original data set head(task_mtcars$row_ids) ## [1] &quot;AMC Javelin&quot; &quot;Cadillac Fleetwood&quot; &quot;Camaro Z28&quot; ## [4] &quot;Chrysler Imperial&quot; &quot;Datsun 710&quot; &quot;Dodge Challenger&quot; # retrieve data for rows with id &quot;Datsun 710&quot; task_mtcars$data(rows = &quot;Datsun 710&quot;) ## mpg cyl disp ## 1: 22.8 4 108 Note that the method $data() is only an accessor and does not modify the underlying data/task. Analogously, each column has an identifier, which is often just called column name. These are stored in the public fields feature_names and target_names: task_iris$feature_names ## [1] &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; task_iris$target_names ## [1] &quot;Species&quot; # retrieve data for rows 1, 51, and 101 and only select column &quot;Species&quot; task_iris$data(rows = c(1, 51, 101), cols = &quot;Species&quot;) ## Species ## 1: setosa ## 2: versicolor ## 3: virginica To retrieve the complete data set, e.g. for a closer inspection, convert to a data.table(): summary(as.data.table(task_iris)) ## Species Petal.Length Petal.Width Sepal.Length ## setosa :50 Min. :1.000 Min. :0.100 Min. :4.300 ## versicolor:50 1st Qu.:1.600 1st Qu.:0.300 1st Qu.:5.100 ## virginica :50 Median :4.350 Median :1.300 Median :5.800 ## Mean :3.758 Mean :1.199 Mean :5.843 ## 3rd Qu.:5.100 3rd Qu.:1.800 3rd Qu.:6.400 ## Max. :6.900 Max. :2.500 Max. :7.900 ## Sepal.Width ## Min. :2.000 ## 1st Qu.:2.800 ## Median :3.000 ## Mean :3.057 ## 3rd Qu.:3.300 ## Max. :4.400 3.1.4.2 Roles It is possible to assign special roles to (subsets of) rows and columns. For example, the previously constructed mtcars task has the following column roles: task_mtcars$col_roles ## $feature ## [1] &quot;cyl&quot; &quot;disp&quot; ## ## $target ## [1] &quot;mpg&quot; ## ## $label ## character(0) ## ## $order ## character(0) ## ## $groups ## character(0) ## ## $weights ## character(0) Now, we want the original rownames() of mtcars to be a regular feature column. Thus, we first pre-process the data.frame and then re-create the task. library(&quot;data.table&quot;) # with `keep.rownames`, data.table stores the row names in an extra column &quot;rn&quot; data = as.data.table(mtcars[, 1:3], keep.rownames = TRUE) task = TaskRegr$new(id = &quot;cars&quot;, backend = data, target = &quot;mpg&quot;) # we now have integer row_ids task$row_ids ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## [24] 24 25 26 27 28 29 30 31 32 # there is a new &quot;feature&quot; called &quot;rn&quot; task$feature_names ## [1] &quot;cyl&quot; &quot;disp&quot; &quot;rn&quot; The column “rn” is now a regular feature. As this is a unique string column, most machine learning algorithms will have problems to process this feature without some kind of preprocessing. However, we still might want to carry rn around for different reasons. E.g., we can use the row names in plots or to associate outliers with the row names. This being said, we need to change the role of the row names column rn and remove it from the set of active features. task$feature_names ## [1] &quot;cyl&quot; &quot;disp&quot; &quot;rn&quot; task$set_col_role(&quot;rn&quot;, new_roles = &quot;label&quot;) # &quot;rn&quot; not listed as feature any more task$feature_names ## [1] &quot;cyl&quot; &quot;disp&quot; # also vanished from &quot;data&quot; and &quot;head&quot; task$data(rows = 1:2) ## mpg cyl disp ## 1: 21 6 160 ## 2: 21 6 160 task$head(2) ## mpg cyl disp ## 1: 21 6 160 ## 2: 21 6 160 Note that no copies of the underlying data is inflicted by this operation. By changing roles, only the view on the data is changed, not the data itself. Just like columns, it is also possible to assign different roles to rows. Rows can have two different roles: Role \"use\": Rows that are generally available for model fitting (although they may also be used as test set in resampling). This is the default role. Role \"validation\": Rows that are held back (see below). Rows which have missing values in the target column upon task creation are automatically moved to the validation set. There are several reasons to hold some observations back or treat them differently: It is often good practice to validate the final model on an external validation set to uncover possible overfitting Some observations may be unlabeled, e.g. in data mining cups or Kaggle competitions. These observations cannot be used for training a model, but you can still predict labels. 3.1.4.3 Task Mutators The methods set_col_role() and set_row_role() change the view on the data and can be used to subset the task. For convenience, the method filter() subsets the task based on row ids, and select() subsets the task based on feature names. All these operations only change the view on the data, without creating a copy of it, but modify the task in-place. task = mlr_tasks$get(&quot;iris&quot;) task$select(c(&quot;Sepal.Width&quot;, &quot;Sepal.Length&quot;)) # keep only these features task$filter(1:3) # keep only these rows task$head() ## Species Sepal.Length Sepal.Width ## 1: setosa 5.1 3.5 ## 2: setosa 4.9 3.0 ## 3: setosa 4.7 3.2 Additionally, the methods rbind() and cbind() allow to add extra rows and columns to a task, respectively. The method replace_features() is a convenience wrapper around select() and cbind(). Again, the original data set stored in the original mlr3::DataBackend is not altered in any way. task$cbind(data.table(foo = letters[1:3])) # add column foo task$head() ## Species Sepal.Length Sepal.Width foo ## 1: setosa 5.1 3.5 a ## 2: setosa 4.9 3.0 b ## 3: setosa 4.7 3.2 c 3.2 Learners Objects of class mlr3::Learner provide a unified interface to many popular machine learning algorithms in R. They consist of methods to train and predict on a mlr3::Task, and additionally provide meta information about the algorithms. The package ships with only a rather minimal set of classification and regression learners, more are implemented in the mlr3learners package. Furthermore, mlr3learners has some documentation on creating custom learners. 3.2.1 Predefined Learners Analogously to mlr3::mlr_tasks, the mlr3::Dictionary mlr3::mlr_learners can be queried for available learners: mlr_learners ## &lt;DictionaryLearner&gt; with 5 stored values ## Keys: classif.debug, classif.featureless, classif.rpart, ## regr.featureless, regr.rpart ## ## Public: add(), get(), has(), items, keys(), mget(), remove() as.data.table(mlr_learners) ## key feature_types ## 1: classif.debug logical,integer,numeric,character,factor,ordered ## 2: classif.featureless logical,integer,numeric,character,factor,ordered ## 3: classif.rpart logical,integer,numeric,character,factor,ordered ## 4: regr.featureless logical,integer,numeric,character,factor,ordered ## 5: regr.rpart logical,integer,numeric,character,factor,ordered ## packages ## 1: ## 2: ## 3: rpart ## 4: ## 5: rpart ## properties ## 1: missings ## 2: importance,missings,multiclass,selected_features,twoclass ## 3: importance,missings,multiclass,selected_features,twoclass,weights ## 4: importance,missings,selected_features ## 5: importance,missings,selected_features,weights ## predict_types ## 1: response,prob ## 2: response,prob ## 3: response,prob ## 4: response,se ## 5: response As listed in the output, each learner comes with the following annotations: feature_types: what kind of features can be processed. packages: which packages are required to run train() and predict(). properties: additional properties and capabilities. E.g., a learner has the property “missings” if it is able to handle missing values natively, and “importance” if it is possible to extract feature importance values. predict_types: what predict types are possible. E.g., a classification learner can predict labels (“response”) or probabilities (“prob”) To extract a specific learner, use the corresponding “id”: learner = mlr_learners$get(&quot;classif.rpart&quot;) learner ## &lt;LearnerClassifRpart:classif.rpart&gt; ## Parameters: list() ## Packages: rpart ## Predict Type: response ## Feature types: logical, integer, numeric, character, factor, ## ordered ## Properties: importance, missings, multiclass, selected_features, ## twoclass, weights ## ## Public: clone(), data_formats, fallback, feature_types, hash, id, ## importance(), model, packages, param_set, params(), ## predict_type, predict_types, predict(), properties, ## selected_features(), task_type, train() As the printer shows, all information from the previous table is also accessible via public fields (id, feature_types, packages, properties, predict_types) Additionally, predict_type returns the currently selected predict type of the learner. The field param_set stores a description of hyperparameter settings: learner$param_set ## ParamSet: paramset ## id class lower upper levels default value ## 1: minsplit ParamInt 1 Inf 20 ## 2: cp ParamDbl 0 1 0.01 ## 3: maxcompete ParamInt 0 Inf 4 ## 4: maxsurrogate ParamInt 0 Inf 5 ## 5: maxdepth ParamInt 1 30 30 ## 6: xval ParamInt 0 Inf 10 The set of hyperparamter values is stored inside the parameter set in the field values. By assigning a named list to this field, we change the active hyperparameters of the learner: learner$param_set$values = list(cp = 0.01) learner ## &lt;LearnerClassifRpart:classif.rpart&gt; ## Parameters: cp=0.01 ## Packages: rpart ## Predict Type: response ## Feature types: logical, integer, numeric, character, factor, ## ordered ## Properties: importance, missings, multiclass, selected_features, ## twoclass, weights ## ## Public: clone(), data_formats, fallback, feature_types, hash, id, ## importance(), model, packages, param_set, params(), ## predict_type, predict_types, predict(), properties, ## selected_features(), task_type, train() The field model stores the result of the training step. As we have not yet learned a model, this is NULL: learner$model ## NULL 3.2.2 Train and Predict Is recommended to train the learner via the mlr3::Experiment class, we only train the learner here directly to showcase more of its API. First, we retrieve the “iris” task from the mlr3::mlr_tasks dictionary, and then apply the train method on the complete task. task = mlr_tasks$get(&quot;iris&quot;) learner$train(task) ## &lt;LearnerClassifRpart:classif.rpart&gt; ## Parameters: cp=0.01 ## Packages: rpart ## Predict Type: response ## Feature types: logical, integer, numeric, character, factor, ## ordered ## Properties: importance, missings, multiclass, selected_features, ## twoclass, weights ## ## Public: clone(), data_formats, fallback, feature_types, hash, id, ## importance(), model, packages, param_set, params(), ## predict_type, predict_types, predict(), properties, ## selected_features(), task_type, train() The learner returns itself, the fitted model is stored in the field “model”: learner$model ## n= 150 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 150 100 setosa (0.33333333 0.33333333 0.33333333) ## 2) Petal.Length&lt; 2.45 50 0 setosa (1.00000000 0.00000000 0.00000000) * ## 3) Petal.Length&gt;=2.45 100 50 versicolor (0.00000000 0.50000000 0.50000000) ## 6) Petal.Width&lt; 1.75 54 5 versicolor (0.00000000 0.90740741 0.09259259) * ## 7) Petal.Width&gt;=1.75 46 1 virginica (0.00000000 0.02173913 0.97826087) * Next, we generate predictions on the complete iris data set: predictions = learner$predict(task) The returned mlr3::PredictionClassif object stores the predicted labels (“response”) as well as the true labels (“truth”). By simply counting the number of correct predictions and dividing by the number of observations, we can calculate the accuracy: tab = as.data.table(predictions) mean(tab$response == tab$truth) ## [1] 0.96 Note that this measure is over-optimistic. As we did not used an independent test set, this is the re-substitution error. 3.3 Performance Measures "],
["experiments.html", "4 Experiments 4.1 Task and learner objects 4.2 Index vector for train/test splits 4.3 Setting up an experiment 4.4 Training 4.5 Predicting", " 4 Experiments In this introduction, we fit a single classification tree on the iris and determine the mean misclassification error. 4.1 Task and learner objects First, we need to generate the following mlr3 objects from the task dictionary and the learner dictionary, respectively: The classification task library(mlr3) task = mlr_tasks$get(&quot;iris&quot;) A learner for the classification tree library(mlr3) learner = mlr_learners$get(&quot;classif.rpart&quot;) 4.2 Index vector for train/test splits We opt to learn on 80% of all available observations and predict on the remaining 20% observations. For this purpose, we create two index vectors: train_set = sample(task$nrow, 0.8 * task$nrow) test_set = setdiff(seq_len(task$nrow), train_set) 4.3 Setting up an experiment The process of fitting a machine learning model, predicting on test data and scoring the predictions by comparing predicted and true labels is called an experiment. For this reason, we start by initializing a new Experiment object by passing the created TaskClassif and LearnerClassif: e = Experiment$new(task = task, learner = learner) print(e) ## &lt;Experiment&gt; [defined]: ## + Task: iris ## + Learner: classif.rpart ## - Model: [missing] ## - Predictions: [missing] ## - Performance: [missing] ## ## Public: clone(), ctrl, data, has_errors, hash, learner, log(), ## model, performance, predict(), prediction, score(), seeds, ## state, task, test_set, timings, train_set, train(), ## validation_set The printer shows a summary of the state of the experiment, which is currently in the state “defined” which basically means that the task and the learner are stored, but nothing else happened so far. By querying the state, the ordered factor levels tell us the other possible states of an experiment: e$state ## [1] defined ## Levels: undefined &lt; defined &lt; trained &lt; predicted &lt; scored 4.4 Training To train the learner on the task, we need to call the train function of the experiment: e$train(row_ids = train_set) ## INFO [mlr3] Training learner &#39;classif.rpart&#39; on task &#39;iris&#39; ... ## &lt;Experiment&gt; [trained]: ## + Task: iris ## + Learner: classif.rpart ## + Model: [rpart] ## - Predictions: [missing] ## - Performance: [missing] ## ## Public: clone(), ctrl, data, has_errors, hash, learner, log(), ## model, performance, predict(), prediction, score(), seeds, ## state, task, test_set, timings, train_set, train(), ## validation_set print(e) ## &lt;Experiment&gt; [trained]: ## + Task: iris ## + Learner: classif.rpart ## + Model: [rpart] ## - Predictions: [missing] ## - Performance: [missing] ## ## Public: clone(), ctrl, data, has_errors, hash, learner, log(), ## model, performance, predict(), prediction, score(), seeds, ## state, task, test_set, timings, train_set, train(), ## validation_set The printer indicates that the Experiment object was modified (its state is now [trained]) and was also extended, since the object now includes a rpart model: rpart.model = e$model print(rpart.model) ## n= 120 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 120 77 virginica (0.32500000 0.31666667 0.35833333) ## 2) Petal.Length&lt; 2.6 39 0 setosa (1.00000000 0.00000000 0.00000000) * ## 3) Petal.Length&gt;=2.6 81 38 virginica (0.00000000 0.46913580 0.53086420) ## 6) Petal.Width&lt; 1.75 41 4 versicolor (0.00000000 0.90243902 0.09756098) * ## 7) Petal.Width&gt;=1.75 40 1 virginica (0.00000000 0.02500000 0.97500000) * 4.5 Predicting After the training step, we can use the experiment to predict on observations of the task (note that you may alternatively also pass new data here as data.frame): e$predict(row_ids = test_set) ## INFO [mlr3] Predicting with model of learner &#39;classif.rpart&#39; on task &#39;iris&#39; ... ## &lt;Experiment&gt; [predicted]: ## + Task: iris ## + Learner: classif.rpart ## + Model: [rpart] ## + Predictions: [PredictionClassif] ## - Performance: [missing] ## ## Public: clone(), ctrl, data, has_errors, hash, learner, log(), ## model, performance, predict(), prediction, score(), seeds, ## state, task, test_set, timings, train_set, train(), ## validation_set print(e) ## &lt;Experiment&gt; [predicted]: ## + Task: iris ## + Learner: classif.rpart ## + Model: [rpart] ## + Predictions: [PredictionClassif] ## - Performance: [missing] ## ## Public: clone(), ctrl, data, has_errors, hash, learner, log(), ## model, performance, predict(), prediction, score(), seeds, ## state, task, test_set, timings, train_set, train(), ## validation_set The predictions can be retrieved as a simple data.table. library(data.table) head(as.data.table(e$prediction)) ## row_id response truth ## 1: 4 setosa setosa ## 2: 21 setosa setosa ## 3: 23 setosa setosa ## 4: 25 setosa setosa ## 5: 27 setosa setosa ## 6: 28 setosa setosa 4.5.1 Performance assessment The last step of the experiment is quantifying the performance of the model by comparing the predicted labels with the true labels using a performance measure. The default measure for the iris classification task is the mean misclassification error, which is used here by default: task$measures[[1L]]$id ## [1] &quot;classif.mmce&quot; e$score() ## INFO [mlr3] Scoring predictions of learner &#39;classif.rpart&#39; on task &#39;iris&#39; ... ## &lt;Experiment&gt; [scored]: ## + Task: iris ## + Learner: classif.rpart ## + Model: [rpart] ## + Predictions: [PredictionClassif] ## + Performance: classif.mmce=0.03333333 ## ## Public: clone(), ctrl, data, has_errors, hash, learner, log(), ## model, performance, predict(), prediction, score(), seeds, ## state, task, test_set, timings, train_set, train(), ## validation_set print(e) ## &lt;Experiment&gt; [scored]: ## + Task: iris ## + Learner: classif.rpart ## + Model: [rpart] ## + Predictions: [PredictionClassif] ## + Performance: classif.mmce=0.03333333 ## ## Public: clone(), ctrl, data, has_errors, hash, learner, log(), ## model, performance, predict(), prediction, score(), seeds, ## state, task, test_set, timings, train_set, train(), ## validation_set e$performance[&quot;classif.mmce&quot;] ## classif.mmce ## 0.03333333 The experiment is now “complete” which means we can access all of its methods. 4.5.2 Chaining methods Instead of calling the methods $train(), $predict() and $score() one after each other, it is also possible to chain these commands: Experiment$new(task = task, learner = learner)$train(train_set)$predict(test_set)$score() ## INFO [mlr3] Training learner &#39;classif.rpart&#39; on task &#39;iris&#39; ... ## INFO [mlr3] Predicting with model of learner &#39;classif.rpart&#39; on task &#39;iris&#39; ... ## INFO [mlr3] Scoring predictions of learner &#39;classif.rpart&#39; on task &#39;iris&#39; ... ## &lt;Experiment&gt; [scored]: ## + Task: iris ## + Learner: classif.rpart ## + Model: [rpart] ## + Predictions: [PredictionClassif] ## + Performance: classif.mmce=0.03333333 ## ## Public: clone(), ctrl, data, has_errors, hash, learner, log(), ## model, performance, predict(), prediction, score(), seeds, ## state, task, test_set, timings, train_set, train(), ## validation_set "],
["resampling.html", "5 Resampling 5.1 Objects 5.2 Resampling 5.3 Manual instantiation 5.4 Custom resampling", " 5 Resampling 5.1 Objects Again, we consider the iris task and a simple classification tree here. library(mlr3) task = mlr_tasks$get(&quot;iris&quot;) learner = mlr_learners$get(&quot;classif.rpart&quot;) Additionally, we need to define how we want to resample. mlr3 comes with the following resampling strategies implemented: mlr_resamplings$keys() ## [1] &quot;bootstrap&quot; &quot;custom&quot; &quot;cv&quot; &quot;holdout&quot; &quot;repeated_cv&quot; ## [6] &quot;subsampling&quot; Additional resampling methods for special use cases will be available via extension packages, such as mlr3spatiotemporal for spatial data (still in development). The experiment conducted in the introduction on train/predict/score is equivalent to a simple “holdout”, so let’s consider this one first. resampling = mlr_resamplings$get(&quot;holdout&quot;) print(resampling) ## &lt;ResamplingHoldout&gt; with 1 iterations ## Instantiated: FALSE ## Parameters: ratio=0.6667 ## ## Public: clone, duplicated_ids, format, hash, id, instance, ## instantiate, is_instantiated, iters, param_set, task_hash, ## test_set, train_set print(resampling$param_set$values) ## $ratio ## [1] 0.6666667 To change the ratio to \\(0.8\\), we simply overwrite the slot: resampling$param_set$values = list(ratio = 0.8) 5.2 Resampling Now, we can pass all created objects to the resample() function to get an object of class ResampleResult: rr = resample(task, learner, resampling) ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;iris&#39; (iteration 1/1)&#39; print(rr) ## &lt;ResampleResult&gt; of learner &#39;iris&#39; on task &#39;classif.rpart&#39; with 1 iterations ## Measure Min. 1st Qu. Median Mean 3rd Qu. Max. Sd ## classif.mmce 0.06667 0.06667 0.06667 0.06667 0.06667 0.06667 NA Before we go into more detail, let’s change the resampling to a 3-fold cross-validation to better illustrate what operations are possible with a resampling result. resampling = mlr_resamplings$get(&quot;cv&quot;, param_vals = list(folds = 3L)) rr = resample(task, learner, resampling) ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;iris&#39; (iteration 1/3)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;iris&#39; (iteration 2/3)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;iris&#39; (iteration 3/3)&#39; print(rr) ## &lt;ResampleResult&gt; of learner &#39;iris&#39; on task &#39;classif.rpart&#39; with 3 iterations ## Measure Min. 1st Qu. Median Mean 3rd Qu. Max. Sd ## classif.mmce 0.04 0.05 0.06 0.05333 0.06 0.06 0.01155 We can do different things with resampling results, e.g.: Extract the performance for the individual resampling iterations: rr$performance(&quot;classif.mmce&quot;) ## [1] 0.04 0.06 0.06 Extract and inspect the now created resampling: rr$resampling ## &lt;ResamplingCV&gt; with 3 iterations ## Instantiated: TRUE ## Parameters: folds=3 ## ## Public: clone, duplicated_ids, format, hash, id, instance, ## instantiate, is_instantiated, iters, param_set, task_hash, ## test_set, train_set rr$resampling$iters ## [1] 3 rr$resampling$test_set(1) ## [1] 4 7 8 13 15 21 31 36 38 39 40 45 47 52 55 58 59 ## [18] 62 70 73 75 78 80 82 83 87 88 89 91 94 95 96 99 100 ## [35] 101 103 105 109 110 117 119 125 132 134 136 137 140 142 144 146 rr$resampling$test_set(2) ## [1] 1 3 6 11 12 14 17 18 19 22 23 24 30 32 33 34 43 ## [18] 44 49 50 51 57 60 64 65 68 69 74 76 77 84 92 93 97 ## [35] 106 107 111 115 118 121 124 126 127 129 130 133 138 141 143 149 rr$resampling$test_set(3) ## [1] 2 5 9 10 16 20 25 26 27 28 29 35 37 41 42 46 48 ## [18] 53 54 56 61 63 66 67 71 72 79 81 85 86 90 98 102 104 ## [35] 108 112 113 114 116 120 122 123 128 131 135 139 145 147 148 150 Retrieve the experiment of a specific iteration and inspect it: e = rr$experiment(iter = 1) e$model ## n= 100 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 100 63 setosa (0.37000000 0.29000000 0.34000000) ## 2) Petal.Length&lt; 2.7 37 0 setosa (1.00000000 0.00000000 0.00000000) * ## 3) Petal.Length&gt;=2.7 63 29 virginica (0.00000000 0.46031746 0.53968254) ## 6) Petal.Width&lt; 1.65 31 3 versicolor (0.00000000 0.90322581 0.09677419) * ## 7) Petal.Width&gt;=1.65 32 1 virginica (0.00000000 0.03125000 0.96875000) * 5.3 Manual instantiation If you want to compare multiple learners, you should use the same resampling per task to reduce the variance of the performance estimation. Until now, we have just passed a resampling strategy to resample(), without specifying the actual splits into training and test. Here, we manually instantiate the resampling: resampling = mlr_resamplings$get(&quot;cv&quot;, param_vals = list(folds = 3L)) resampling$instantiate(task) resampling$iters ## [1] 3 resampling$train_set(1) ## [1] 1 10 11 12 14 17 18 20 21 25 29 32 34 37 41 45 46 ## [18] 51 53 55 57 58 60 63 68 69 71 78 90 92 93 94 97 98 ## [35] 102 105 111 114 117 118 120 121 122 124 129 132 135 143 144 147 3 ## [52] 5 6 7 8 9 16 19 23 24 27 30 31 33 38 40 42 49 ## [69] 52 54 56 62 64 65 70 72 74 76 80 84 85 87 101 103 106 ## [86] 109 115 119 127 130 131 133 134 136 139 141 142 145 146 148 If we now pass this instantiated object to resample, the pre-calculated training and test splits will be used for both learners: learner1 = mlr_learners$get(&quot;classif.rpart&quot;) # simple classification tree learner2 = mlr_learners$get(&quot;classif.featureless&quot;) # featureless learner, prediction majority class rr1 = resample(task, learner1, resampling) ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;iris&#39; (iteration 1/3)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;iris&#39; (iteration 2/3)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;iris&#39; (iteration 3/3)&#39; rr2 = resample(task, learner2, resampling) ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;iris&#39; (iteration 1/3)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;iris&#39; (iteration 2/3)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;iris&#39; (iteration 3/3)&#39; setequal(rr1$experiment(1)$train_set, rr2$experiment(1)$train_set) ## [1] TRUE We can also combine the created result objects into a BenchmarkResult (see below for an introduction to simple benchmarking): bmr = rr1$combine(rr2) bmr$aggregated(objects = FALSE) ## hash resampling_id task_id learner_id classif.mmce ## 1: 0ca42071834e6eca cv iris classif.rpart 0.06666667 ## 2: 1a7bccdfa62ec084 cv iris classif.featureless 0.70000000 5.4 Custom resampling Sometimes it is necessary to perform resampling with custom splits, e.g. to reproduce a study. For this purpose, splits can be manually set for ResamplingCustom: resampling = mlr_resamplings$get(&quot;custom&quot;) resampling$instantiate(task, list(c(1:10, 51:60, 101:110)), list(c(11:20, 61:70, 111:120)) ) resampling$iters ## [1] 1 resampling$train_set(1) ## [1] 1 2 3 4 5 6 7 8 9 10 51 52 53 54 55 56 57 ## [18] 58 59 60 101 102 103 104 105 106 107 108 109 110 resampling$test_set(1) ## [1] 11 12 13 14 15 16 17 18 19 20 61 62 63 64 65 66 67 ## [18] 68 69 70 111 112 113 114 115 116 117 118 119 120 "],
["benchmarking.html", "6 Benchmarking 6.1 Benchmarking Exhaustive Designs 6.2 Converting specific benchmark objects to resample objects", " 6 Benchmarking Comparing the performance of different learners on multiple tasks is a recurrent task. mlr3 offers the benchmark() function for convenience. 6.1 Benchmarking Exhaustive Designs The interface of the benchmark() function accepts a design of tasks, learners, and resampling strategies as data frame. Here, we call benchmark() to perform a single holdout split on a single task and two learners: library(data.table) design = data.table( task = mlr_tasks$mget(&quot;iris&quot;), learner = mlr_learners$mget(c(&quot;classif.rpart&quot;, &quot;classif.featureless&quot;)), resampling = mlr_resamplings$mget(&quot;holdout&quot;) ) print(design) ## task learner resampling ## 1: &lt;TaskClassif&gt; &lt;LearnerClassifRpart&gt; &lt;ResamplingHoldout&gt; ## 2: &lt;TaskClassif&gt; &lt;LearnerClassifFeatureless&gt; &lt;ResamplingHoldout&gt; bmr = benchmark(design) ## INFO [mlr3] Benchmarking 2 experiments ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;iris&#39; (iteration 1/1)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;iris&#39; (iteration 1/1)&#39; ## INFO [mlr3] Finished benchmark Note that the holdout splits have been automatically instantiated for each row of the design. As a result, the rpart learner used a different training set than the featureless learner. However, for comparison of learners you usually want the learners to see the same splits into train and test sets. To overcome this issue, the resampling strategy needs to be manually instantiated before creating the design. While the interface of benchmark() allows full flexibility, the creation of such design tables can be tedious. Therefore, mlr3 provides a helper function to quickly generate design tables and instantiate resampling strategies in an exhaustive grid fashion: mlr3::expand_grid(). # get some example tasks tasks = mlr_tasks$mget(c(&quot;pima&quot;, &quot;sonar&quot;, &quot;spam&quot;)) # set measures for all tasks: accuracy (acc) and area under the curve (auc) measures = mlr_measures$mget(c(&quot;classif.acc&quot;, &quot;classif.auc&quot;)) tasks = lapply(tasks, function(task) { task$measures = measures; task }) # get a featureless learner and a classification tree learners = mlr_learners$mget(c(&quot;classif.featureless&quot;, &quot;classif.rpart&quot;)) # let the learners predict probabilities instead of class labels (required for AUC measure) learners$classif.featureless$predict_type = &quot;prob&quot; learners$classif.rpart$predict_type = &quot;prob&quot; # compare via 10-fold cross validation resamplings = mlr_resamplings$mget(&quot;cv&quot;) # create a BenchmarkResult object design = expand_grid(tasks, learners, resamplings) print(design) ## task learner resampling ## 1: &lt;TaskClassif&gt; &lt;LearnerClassifFeatureless&gt; &lt;ResamplingCV&gt; ## 2: &lt;TaskClassif&gt; &lt;LearnerClassifRpart&gt; &lt;ResamplingCV&gt; ## 3: &lt;TaskClassif&gt; &lt;LearnerClassifFeatureless&gt; &lt;ResamplingCV&gt; ## 4: &lt;TaskClassif&gt; &lt;LearnerClassifRpart&gt; &lt;ResamplingCV&gt; ## 5: &lt;TaskClassif&gt; &lt;LearnerClassifFeatureless&gt; &lt;ResamplingCV&gt; ## 6: &lt;TaskClassif&gt; &lt;LearnerClassifRpart&gt; &lt;ResamplingCV&gt; bmr = benchmark(design) ## INFO [mlr3] Benchmarking 60 experiments ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;pima&#39; (iteration 1/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;pima&#39; (iteration 2/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;pima&#39; (iteration 3/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;pima&#39; (iteration 4/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;pima&#39; (iteration 5/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;pima&#39; (iteration 6/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;pima&#39; (iteration 7/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;pima&#39; (iteration 8/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;pima&#39; (iteration 9/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;pima&#39; (iteration 10/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;pima&#39; (iteration 1/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;pima&#39; (iteration 2/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;pima&#39; (iteration 3/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;pima&#39; (iteration 4/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;pima&#39; (iteration 5/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;pima&#39; (iteration 6/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;pima&#39; (iteration 7/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;pima&#39; (iteration 8/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;pima&#39; (iteration 9/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;pima&#39; (iteration 10/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;sonar&#39; (iteration 1/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;sonar&#39; (iteration 2/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;sonar&#39; (iteration 3/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;sonar&#39; (iteration 4/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;sonar&#39; (iteration 5/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;sonar&#39; (iteration 6/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;sonar&#39; (iteration 7/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;sonar&#39; (iteration 8/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;sonar&#39; (iteration 9/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;sonar&#39; (iteration 10/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;sonar&#39; (iteration 1/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;sonar&#39; (iteration 2/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;sonar&#39; (iteration 3/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;sonar&#39; (iteration 4/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;sonar&#39; (iteration 5/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;sonar&#39; (iteration 6/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;sonar&#39; (iteration 7/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;sonar&#39; (iteration 8/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;sonar&#39; (iteration 9/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;sonar&#39; (iteration 10/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;spam&#39; (iteration 1/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;spam&#39; (iteration 2/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;spam&#39; (iteration 3/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;spam&#39; (iteration 4/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;spam&#39; (iteration 5/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;spam&#39; (iteration 6/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;spam&#39; (iteration 7/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;spam&#39; (iteration 8/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;spam&#39; (iteration 9/10)&#39; ## INFO [mlr3] Running learner &#39;classif.featureless&#39; on task &#39;spam&#39; (iteration 10/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;spam&#39; (iteration 1/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;spam&#39; (iteration 2/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;spam&#39; (iteration 3/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;spam&#39; (iteration 4/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;spam&#39; (iteration 5/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;spam&#39; (iteration 6/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;spam&#39; (iteration 7/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;spam&#39; (iteration 8/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;spam&#39; (iteration 9/10)&#39; ## INFO [mlr3] Running learner &#39;classif.rpart&#39; on task &#39;spam&#39; (iteration 10/10)&#39; ## INFO [mlr3] Finished benchmark The aggregated resampling results can be accessed with: bmr$aggregated(objects = FALSE) ## hash resampling_id task_id learner_id classif.acc ## 1: 94d902aac8167d43 cv pima classif.featureless 0.6509740 ## 2: 0129a8f46ca57000 cv pima classif.rpart 0.7449590 ## 3: a1b5c4f2c511e734 cv sonar classif.featureless 0.5333333 ## 4: ddd4db8aba8a10de cv sonar classif.rpart 0.7159524 ## 5: 37bc4caf30a60611 cv spam classif.featureless 0.6059563 ## 6: 44e9a0aa0eb377d9 cv spam classif.rpart 0.8926294 ## classif.auc ## 1: 0.5000000 ## 2: 0.7973900 ## 3: 0.5000000 ## 4: 0.7493940 ## 5: 0.5000000 ## 6: 0.8902386 We can aggregate it further, i.e. if we are interested which learner performed best over all tasks: bmr$aggregated(objects = FALSE)[, list(acc = mean(classif.acc), auc = mean(classif.auc)), by = &quot;learner_id&quot;] ## learner_id acc auc ## 1: classif.featureless 0.5967546 0.5000000 ## 2: classif.rpart 0.7845136 0.8123409 Unsurprisingly, the classification tree outperformed the featureless learner. 6.2 Converting specific benchmark objects to resample objects As a BenchmarkResult object is basically a collection of multiple ResampleResult objects, we can extract specific ResampleResult objects using the stored hashes: tab = bmr$aggregated(objects = FALSE)[task_id == &quot;spam&quot; &amp; learner_id == &quot;classif.rpart&quot;] print(tab) ## hash resampling_id task_id learner_id classif.acc ## 1: 44e9a0aa0eb377d9 cv spam classif.rpart 0.8926294 ## classif.auc ## 1: 0.8902386 rr = bmr$resample_result(tab$hash) print(rr) ## &lt;ResampleResult&gt; of learner &#39;spam&#39; on task &#39;classif.rpart&#39; with 10 iterations ## Measure Min. 1st Qu. Median Mean 3rd Qu. Max. Sd ## classif.acc 0.8652 0.8712 0.9023 0.8926 0.9065 0.9152 0.01986 ## classif.auc 0.8447 0.8737 0.8922 0.8902 0.9090 0.9212 0.02428 We can now investigate this resampling and even single experiments using the previously introduced API: rr$aggregated ## classif.acc classif.auc ## 0.8926294 0.8902386 # get the iteration with worst AUC worst = as.data.table(rr)[which.min(classif.auc), c(&quot;iteration&quot;, &quot;classif.auc&quot;)] print(worst) ## iteration classif.auc ## 1: 5 0.8446571 # get the corresponding experiment e = rr$experiment(worst$iteration) print(e) ## &lt;Experiment&gt; [scored]: ## + Task: spam ## + Learner: classif.rpart ## + Model: [rpart] ## + Predictions: [PredictionClassif] ## + Performance: classif.acc=0.8652174, classif.auc=0.8446571 ## ## Public: clone(), ctrl, data, has_errors, hash, learner, log(), ## model, performance, predict(), prediction, score(), seeds, ## state, task, test_set, timings, train_set, train(), ## validation_set "],
["error-handling.html", "7 Error Handling 7.1 Setup 7.2 No error handling 7.3 Encapsulation 7.4 Fallback learners", " 7 Error Handling This vignettes demonstrates how to deal with learners which raise exceptions during train or predict. 7.1 Setup First, we need a simple learning task and a learner which raises exceptions. For this purpose, mlr3 ships with the learner classif.debug: task = mlr_tasks$get(&quot;spam&quot;) learner = mlr_learners$get(&quot;classif.debug&quot;) print(learner) ## &lt;LearnerClassifDebug:classif.debug&gt; ## Parameters: list() ## Packages: - ## Predict Type: response ## Feature types: logical, integer, numeric, character, factor, ## ordered ## Properties: missings ## ## Public: clone(), data_formats, fallback, feature_types, hash, id, ## model, packages, param_set, params(), predict_type, ## predict_types, predict(), properties, task_type, train() The hyperparameters let us control (a) what conditions should be signaled (message, warning, error), and (b) during which stage (train or predict). Additionally, we can tell the learner to provoke a segfault which tears down the complete R session. With its default settings, it will do nothing special: it learns a random label and which is used to create constant predictions. 7.2 No error handling In the defaults, mlr3 does not handle errors. Thus, the exception raised by the unittest learner stops the execution and can be tracebacked: task = mlr_tasks$get(&quot;spam&quot;) learner = mlr_learners$get(&quot;classif.debug&quot;) learner$param_set$values = list(error_train = TRUE) e = Experiment$new(task, learner) e$train() ## INFO [mlr3] Training learner &#39;classif.debug&#39; on task &#39;spam&#39; ... ## Error in learner$train(task): Error from classif.debug-&gt;train() 7.3 Encapsulation During parallelization, error messages (as well as normal output or warnings) are often not properly forwarded to the master R session, or they arrive in a confusing order. The learner execution can be encapsulated, so its output is logged to the experiment instead of just printed to the console: task = mlr_tasks$get(&quot;spam&quot;) learner = mlr_learners$get(&quot;classif.debug&quot;) learner$param_set$values = list(warning_train = TRUE, error_train = TRUE) ctrl = mlr_control(encapsulate_train = &quot;evaluate&quot;) e = Experiment$new(task, learner, ctrl = ctrl) e$train() ## INFO [mlr3] Training learner &#39;classif.debug&#39; on task &#39;spam&#39; ... ## &lt;Experiment&gt; [trained]: ## + Task: spam ## + Learner: classif.debug ## - Model: [missing] ## - Predictions: [missing] ## - Performance: [missing] ## ## Public: clone(), ctrl, data, has_errors, hash, learner, log(), ## model, performance, predict(), prediction, score(), seeds, ## state, task, test_set, timings, train_set, train(), ## validation_set e$has_errors # any errors recorded? ## [1] TRUE e$log(&quot;train&quot;) # print train log ## &lt;Log&gt; with 2 messages: ## [WRN] train: Warning from classif.debug-&gt;train() ## [ERR] train: Error from classif.debug-&gt;train() e$log(&quot;train&quot;)$warnings # get all the warnings ## [1] &quot;Warning from classif.debug-&gt;train()&quot; e$log(&quot;train&quot;)$errors # get all the errors ## [1] &quot;Error from classif.debug-&gt;train()&quot; You can also enable the encapsulation for the predict step of an experiment by setting encapsulate_predict. Another possibility to encapsulate is execution via package the callr. callr spawns a new R process, and thus guards us from segfaults. On the downside, starting new processes comes with a computational overhead. ctrl = mlr_control(encapsulate_train = &quot;callr&quot;) e = Experiment$new(task, learner) e$train(ctrl = ctrl) ## INFO [mlr3] Training learner &#39;classif.debug&#39; on task &#39;spam&#39; ... ## &lt;Experiment&gt; [trained]: ## + Task: spam ## + Learner: classif.debug ## - Model: [missing] ## - Predictions: [missing] ## - Performance: [missing] ## ## Public: clone(), ctrl, data, has_errors, hash, learner, log(), ## model, performance, predict(), prediction, score(), seeds, ## state, task, test_set, timings, train_set, train(), ## validation_set e$has_errors ## [1] TRUE e$log(&quot;train&quot;) ## &lt;Log&gt; with 2 messages: ## [WRN] train: Warning from classif.debug-&gt;train() ## [ERR] train: Error from classif.debug-&gt;train() ctrl = mlr_control(encapsulate_train = &quot;callr&quot;) task = mlr_tasks$get(&quot;spam&quot;) learner = mlr_learners$get(&quot;classif.debug&quot;) learner$param_set$values = list(segfault_train = TRUE) e = Experiment$new(task, learner) e$train(ctrl = ctrl) ## INFO [mlr3] Training learner &#39;classif.debug&#39; on task &#39;spam&#39; ... ## &lt;Experiment&gt; [trained]: ## + Task: spam ## + Learner: classif.debug ## - Model: [missing] ## - Predictions: [missing] ## - Performance: [missing] ## ## Public: clone(), ctrl, data, has_errors, hash, learner, log(), ## model, performance, predict(), prediction, score(), seeds, ## state, task, test_set, timings, train_set, train(), ## validation_set e$has_errors ## [1] TRUE e$log(&quot;train&quot;)$errors ## [1] &quot;callr exited with status -11&quot; Note that, although no exception has been raised with encapsulation, it is impossible to perform the predict step without a model: e$predict() ## INFO [mlr3] Predicting with model of learner &#39;classif.debug&#39; on task &#39;spam&#39; ... ## Error in unique(assert_character(pkgs, any.missing = FALSE)): Assertion on &#39;pkgs&#39; failed: Must be of type &#39;character&#39;, not &#39;NULL&#39;. As a workaround, we define a learner in the next section which is used as a surrogate to create predictions. 7.4 Fallback learners Each learner can have a fallback learner, which is used if either the train or predict step fail. Here, we simply fallback to the predictions of a featureless learner (predicting majority class): task = mlr_tasks$get(&quot;spam&quot;) learner = mlr_learners$get(&quot;classif.debug&quot;) learner$param_set$values = list(error_train = TRUE) learner$fallback = mlr_learners$get(&quot;classif.featureless&quot;) ctrl = mlr_control(encapsulate_train = &quot;evaluate&quot;) e = Experiment$new(task = task, learner = learner, ctrl = ctrl) e$train() ## INFO [mlr3] Training learner &#39;classif.debug&#39; on task &#39;spam&#39; ... ## &lt;Experiment&gt; [trained]: ## + Task: spam ## + Learner: classif.debug ## + Model: [featureless] ## - Predictions: [missing] ## - Performance: [missing] ## ## Public: clone(), ctrl, data, has_errors, hash, learner, log(), ## model, performance, predict(), prediction, score(), seeds, ## state, task, test_set, timings, train_set, train(), ## validation_set e$has_errors ## [1] TRUE e$log(&quot;train&quot;) ## &lt;Log&gt; with 1 message: ## [ERR] train: Error from classif.debug-&gt;train() e$predict() ## INFO [mlr3] Predicting with model of learner &#39;classif.debug&#39; on task &#39;spam&#39; ... ## &lt;Experiment&gt; [predicted]: ## + Task: spam ## + Learner: classif.debug ## + Model: [featureless] ## + Predictions: [PredictionClassif] ## - Performance: [missing] ## ## Public: clone(), ctrl, data, has_errors, hash, learner, log(), ## model, performance, predict(), prediction, score(), seeds, ## state, task, test_set, timings, train_set, train(), ## validation_set e$score() ## INFO [mlr3] Scoring predictions of learner &#39;classif.debug&#39; on task &#39;spam&#39; ... ## &lt;Experiment&gt; [scored]: ## + Task: spam ## + Learner: classif.debug ## + Model: [featureless] ## + Predictions: [PredictionClassif] ## + Performance: classif.mmce=0.3940448 ## ## Public: clone(), ctrl, data, has_errors, hash, learner, log(), ## model, performance, predict(), prediction, score(), seeds, ## state, task, test_set, timings, train_set, train(), ## validation_set e$prediction ## &lt;PredictionClassif&gt; for 4601 observations: ## row_id response truth ## 1: 1 nonspam spam ## 2: 2 nonspam spam ## 3: 3 nonspam spam ## --- ## 4599: 4599 nonspam nonspam ## 4600: 4600 nonspam nonspam ## 4601: 4601 nonspam nonspam e$performance ## classif.mmce ## 0.3940448 Note that the logs and timings are tracked for the original learner (until it errored), not the fallback learner. "],
["parallelization.html", "8 Parallelization 8.1 Prerequisites 8.2 Parallel resampling", " 8 Parallelization 8.1 Prerequisites Make sure you have installed future and future.apply: if (!requireNamespace(&quot;future&quot;)) install.packages(&quot;future&quot;) if (!requireNamespace(&quot;future.apply&quot;)) install.packages(&quot;future.apply&quot;) 8.2 Parallel resampling The most outer loop in resampling runs independent repetitions of applying a learner on a subset of a task, predict on a different subset and score the performance by comparing true and predicted labels. This loop is what is called embarrassingly parallel. In the following, we will consider the spam task and a simple classification tree (\"classif.rpart\") to illustrate the parallelization. library(&quot;mlr3&quot;) task = mlr_tasks$get(&quot;spam&quot;) learner = mlr_learners$get(&quot;classif.rpart&quot;) resampling = mlr_resamplings$get(&quot;subsampling&quot;) system.time( resample(task, learner, resampling) )[3L] We now use the future package to parallelize the resampling by selecting a backend via the function plan and then repeat the resampling. We use the “multiprocess” backend here which uses threads on linux/mac and a socket cluster on windows: future::plan(&quot;multiprocess&quot;) system.time( resample(task, learner, resampling) )[3L] On most systems you should see a decrease in the reported real CPU time. On some systems (e.g. windows), the overhead for parallelization is quite large though. Therefore, you should only enable parallelization for experiments which run more than 10s each. Benchmarking is also parallelized. The following code sends 64 jobs (4 tasks * 16 resampling repeats) to the future backend: tasks = mlr_tasks$mget(c(&quot;iris&quot;, &quot;spam&quot;, &quot;pima&quot;)) learners = mlr_learners$mget(&quot;classif.rpart&quot;) resamplings = mlr_resamplings$mget(&quot;subsampling&quot;, param_vals = list(ratio = 0.8, repeats = 16)) future::plan(&quot;multiprocess&quot;) system.time( benchmark(expand_grid(tasks, learners, resamplings)) )[3L] "],
["references.html", "References", " References "]
]
